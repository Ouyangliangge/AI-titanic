{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4274b669",
   "metadata": {},
   "source": [
    "包含一个输入层，一个隐藏层，一个输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d2841bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/82213/Desktop/uniqueAI/泰坦尼克号数据.csv\")     #加载数据\n",
    "#数据处理\n",
    "data['Cabin']=data['Cabin'].fillna('Unknown')\n",
    "data['Embarked']=data['Embarked'].fillna('S')\n",
    "#数据标准化\n",
    "def standardize(data):   #对单一特征进行标准化\n",
    "    mean=np.mean(data,axis=0)    #均值\n",
    "    std=np.std(data,axis=0)   #标准差\n",
    "    standardized_data=(data-mean)/std\n",
    "    return standardized_data\n",
    "\n",
    "#数据归一化\n",
    "def normalize(data):\n",
    "    min_val=np.min(data,axis=0)\n",
    "    max_val=np.max(data,axis=0)\n",
    "    normalized_data=(data-min_val)/(max_val-min_val)\n",
    "    return normalized_data\n",
    "\n",
    "need_data=['Survived','Pclass','SibSp','Parch','Fare']   #Age还未填充\n",
    "for feature in need_data:\n",
    "    DATA=data[feature].values\n",
    "    normalized_data=normalize(DATA) #归一化\n",
    "    final_data=standardize(normalized_data) #标准化\n",
    "    data[feature]=final_data\n",
    "    print(np.isclose(np.mean(final_data,axis=0),0))\n",
    "    \n",
    "def one_hot_encoder(data):\n",
    "    unique_values=np.unique(data) #去除重复元素并进行排序\n",
    "    encoded_data=np.zeros((len(data),len(unique_values))) #创建二维数组\n",
    "    for i in range(len(data)):\n",
    "        value=data[i]\n",
    "        index=np.where(unique_values==value)[0][0]  #获取uniquevalues中与data[i]相等的元素的索引\n",
    "        encoded_data[i,index]=1  #进行独热编码\n",
    "    return encoded_data\n",
    "features=['Name','Sex','Ticket','Cabin','Embarked']\n",
    "for x in features:\n",
    "    DATA=data[x].values\n",
    "    encoded_data=one_hot_encoder(DATA)\n",
    "    data[x]=encoded_data\n",
    "    \n",
    "#age缺失值处理\n",
    "#采用KNN填补\n",
    "def EulcDist(x0,x1):\n",
    "    return np.sqrt(np.sum((x1 - x0) ** 2))    #计算欧氏距离\n",
    "    \n",
    "def knn_impute(df,target_feature,k):\n",
    "    #X是样本特征，y是目标标记，k是最近邻居数量\n",
    "    X=df.drop(columns=[target_feature]).values\n",
    "    y=df[target_feature].values\n",
    "    n_samples,n_features=X.shape    #n_samples是样本数量，n_features是特征数量\n",
    "    #创建两个布尔数组分别表示缺失值和非缺失值的索引，用于获取缺失行和未缺失行\n",
    "    missing_indices=np.isnan(y)\n",
    "    non_missing_indices=~missing_indices \n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if np.isnan(y[i]):    #判断数据是否缺失\n",
    "            X_missing = X[i]   #取当前缺失行的其他特征值\n",
    "            X_non_missing = X[non_missing_indices]  #得到未缺失行的其他特征\n",
    "            distances=[]   #创建距离列表，每个元素为（距离的值，邻居索引）\n",
    "            \n",
    "            for j in range(len(X_non_missing)):   #迭代未缺失行取均值\n",
    "                if i==j:\n",
    "                    continue\n",
    "                dist=EulcDist(X_non_missing[j],X_missing)\n",
    "                distances.append((dist,j))\n",
    "            \n",
    "            distances.sort()\n",
    "            knn_indices= [idx for (_,idx) in distances[:k]]     #从距离列表 distances 中选择距离当前样本最近的 k 个邻居的索引\n",
    "            y[i]=np.mean(y[non_missing_indices][knn_indices])         #先获取未缺失列表，再获取最近邻居值，最后求均值用于填充\n",
    "    df[target_feature]=y\n",
    "    return df\n",
    "data=knn_impute(data,'Age',3)\n",
    "#对age进行标准化、归一化\n",
    "DATA=data['Age'].values\n",
    "normalized_data=normalize(DATA) #归一化\n",
    "final_data=standardize(normalized_data) #标准化\n",
    "data['Age']=final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3153e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f1a9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数\n",
    "def initialize_parameters(input_dim,hidden_dim,output_dim):\n",
    "    #初始化网络的参数，并随机初始化权重矩阵，并将偏置设置为0\n",
    "    np.random.seed(0) #统一设置随机种子、\n",
    "    W1=np.random.randn(hidden_dim,input_dim)*0.01\n",
    "    b1=np.zeros((hidden_dim,1))\n",
    "    W2=np.random.randn(output_dim,hidden_dim)*0.01\n",
    "    b2=np.zeros((output_dim,1))\n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f49f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正向传播\n",
    "def relu(z):\n",
    "    #实现relu激活函数\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    #实现sigmoid激活函数\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, parameters, keep_prob=1.0):\n",
    "    W1, b1, W2, b2 = parameters\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Dropout\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1]) < keep_prob\n",
    "    A1 = A1 * D1 / keep_prob\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"D1\": D1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52a8a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算损失函数\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(Y, np.log(A2)) + np.multiply(1 - Y, np.log(1 - A2))\n",
    "    loss = -np.sum(logprobs) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a99223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#反向传播\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y, keep_prob=1.0):\n",
    "    m = X.shape[1]\n",
    "    W1, b1, W2, b2 = parameters\n",
    "    A1, D1, A2 = cache[\"A1\"], cache[\"D1\"], cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 = dA1 * D1 / keep_prob\n",
    "    dZ1 = relu_backward(dA1, cache[\"Z1\"])\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ea490ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新参数\n",
    "def update_parameters(parameters, gradients, learning_rate, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    W1, b1, W2, b2 = parameters\n",
    "    dW1, db1, dW2, db2 = gradients\n",
    "    \n",
    "    # SGD\n",
    "    # W1 = W1 - learning_rate * dW1\n",
    "    # b1 = b1 - learning_rate * db1\n",
    "    # W2 = W2 - learning_rate * dW2\n",
    "    # b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Adam\n",
    "    v_dW1 = np.zeros_like(dW1)\n",
    "    v_db1 = np.zeros_like(db1)\n",
    "    v_dW2 = np.zeros_like(dW2)\n",
    "    v_db2 = np.zeros_like(db2)\n",
    "    s_dW1 = np.zeros_like(dW1)\n",
    "    s_db1 = np.zeros_like(db1)\n",
    "    s_dW2 = np.zeros_like(dW2)\n",
    "    s_db2 = np.zeros_like(db2)\n",
    "    \n",
    "    v_dW1 = beta1 * v_dW1 + (1 - beta1) * dW1\n",
    "    v_db1 = beta1 * v_db1 + (1 - beta1) * db1\n",
    "    v_dW2 = beta1 * v_dW2 + (1 - beta1) * dW2\n",
    "    v_db2 = beta1 * v_db2 + (1 - beta1) * db2\n",
    "\n",
    "    s_dW1 = beta2 * s_dW1 + (1 - beta2) * (dW1 ** 2)\n",
    "    s_db1 = beta2 * s_db1 + (1 - beta2) * (db1 ** 2)\n",
    "    s_dW2 = beta2 * s_dW2 + (1 - beta2) * (dW2 ** 2)\n",
    "    s_db2 = beta2 * s_db2 + (1 - beta2) * (db2 ** 2)\n",
    "\n",
    "    v_corrected_dW1 = v_dW1 / (1 - beta1 ** t)\n",
    "    v_corrected_db1 = v_db1 / (1 - beta1 ** t)\n",
    "    v_corrected_dW2 = v_dW2 / (1 - beta1 ** t)\n",
    "    v_corrected_db2 = v_db2 / (1 - beta1 ** t)\n",
    "\n",
    "    s_corrected_dW1 = s_dW1 / (1 - beta2 ** t)\n",
    "    s_corrected_db1 = s_db1 / (1 - beta2 ** t)\n",
    "    s_corrected_dW2 = s_dW2 / (1 - beta2 ** t)\n",
    "    s_corrected_db2 = s_db2 / (1 - beta2 ** t)\n",
    "\n",
    "    W1 = W1 - learning_rate * v_corrected_dW1 / (np.sqrt(s_corrected_dW1) + epsilon)\n",
    "    b1 = b1 - learning_rate * v_corrected_db1 / (np.sqrt(s_corrected_db1) + epsilon)\n",
    "    W2 = W2 - learning_rate * v_corrected_dW2 / (np.sqrt(s_corrected_dW2) + epsilon)\n",
    "    b2 = b2 - learning_rate * v_corrected_db2 / (np.sqrt(s_corrected_db2) + epsilon)\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6182555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建神经网络\n",
    "def two_layer_neural_network(X, Y, hidden_dim, num_iterations=1000, learning_rate=0.01, keep_prob=1.0, optimizer=\"sgd\"):\n",
    "    input_dim = X.shape[0]\n",
    "    output_dim = Y.shape[0]\n",
    "    \n",
    "    parameters = initialize_parameters(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    for i in range(1, num_iterations + 1):\n",
    "        A2, cache = forward_propagation(X, parameters, keep_prob)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        gradients = backward_propagation(parameters, cache, X, Y, keep_prob)\n",
    "        \n",
    "        t = i  # 用于Adam优化算法\n",
    "        if optimizer == \"sgd\":\n",
    "            parameters = update_parameters(parameters, gradients, learning_rate, t)\n",
    "        elif optimizer == \"adam\":\n",
    "            parameters = update_parameters(parameters, gradients, learning_rate, t)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Loss after iteration {i}: {loss}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b66f896c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U2'), dtype('<U32')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m X_train, X_test, Y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 使用SGD优化算法，设置保留比例为0.8\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m parameters_sgd \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_layer_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 使用Adam优化算法，设置保留比例为0.8\u001b[39;00m\n\u001b[0;32m     18\u001b[0m parameters_adam \u001b[38;5;241m=\u001b[39m two_layer_neural_network(X_train, Y_train, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, keep_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[63], line 9\u001b[0m, in \u001b[0;36mtwo_layer_neural_network\u001b[1;34m(X, Y, hidden_dim, num_iterations, learning_rate, keep_prob, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m parameters \u001b[38;5;241m=\u001b[39m initialize_parameters(input_dim, hidden_dim, output_dim)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     A2, cache \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(A2, Y)\n\u001b[0;32m     11\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m backward_propagation(parameters, cache, X, Y, keep_prob)\n",
      "Cell \u001b[1;32mIn[59], line 13\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters, keep_prob)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X, parameters, keep_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m     11\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m parameters\n\u001b[1;32m---> 13\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m     14\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m relu(Z1)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Dropout\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U2'), dtype('<U32')) -> None"
     ]
    }
   ],
   "source": [
    "#进行预测\n",
    "def predict(X, parameters):\n",
    "    A2, _ = forward_propagation(X, parameters, keep_prob=1.0)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "X=data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']].values\n",
    "y=data['Survived'].values\n",
    "#切割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#将 80% 的数据作为训练集，20% 的数据作为测试集\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用SGD优化算法，设置保留比例为0.8\n",
    "parameters_sgd = two_layer_neural_network(X_train, Y_train, hidden_dim=50, num_iterations=1000, learning_rate=0.01, keep_prob=0.8, optimizer=\"sgd\")\n",
    "\n",
    "# 使用Adam优化算法，设置保留比例为0.8\n",
    "parameters_adam = two_layer_neural_network(X_train, Y_train, hidden_dim=50, num_iterations=1000, learning_rate=0.01, keep_prob=0.8, optimizer=\"adam\")\n",
    "\n",
    "# SGD优化算法的预测结果\n",
    "predictions_sgd = predict(X_test, parameters_sgd)\n",
    "\n",
    "# Adam优化算法的预测结果\n",
    "predictions_adam = predict(X_test, parameters_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f28079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995d9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
